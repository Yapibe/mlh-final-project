{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Project Setup and Authentication\n",
    "\n",
    "**Objective:** Configure the environment and authenticate with Google Cloud to access the MIMIC-III dataset on BigQuery.\n",
    "\n",
    "This notebook handles the necessary setup for running the project in both Google Colab and a local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# A robust way to check if we are in Google Colab\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    from google.colab import data_table\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure Google Cloud Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'deep-atlas-413309' # Yair Bigquery Project ID\n",
    "# project_id = 'light-legend-457315-k3' # Lital Bigquery Project ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Authenticate and Initialize Client\n",
    "\n",
    "The following cell handles authentication. \n",
    "- **In Google Colab:** It will prompt you to authenticate with your Google account.\n",
    "- **Locally:** It will use the credentials from the `gcloud` CLI. If you haven't authenticated locally before, please run the following command in your terminal and follow the instructions:\n",
    "\n",
    "  ```bash\n",
    "  gcloud auth application-default login\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local Google Cloud SDK credentials.\n",
      "BigQuery client initialized for project: deep-atlas-413309\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Authenticating in Google Colab...\")\n",
    "    auth.authenticate_user()\n",
    "    data_table.enable_dataframe_formatter()\n",
    "else:\n",
    "    print(\"Using local Google Cloud SDK credentials.\")\n",
    "\n",
    "client = bigquery.Client(project=project_id)\n",
    "print(f\"BigQuery client initialized for project: {client.project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Display Database Schema (Optional)\n",
    "\n",
    "The following cell queries the `INFORMATION_SCHEMA` of the `physionet-data.mimiciii_clinical` dataset to display all tables and their respective columns. This is a useful for understanding the database structure directly from the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pickh\\pythonProject\\MLH\\.venv\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:1957: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MIMIC-III Clinical Dataset Schema ---\n",
      "\n",
      "-- Table: admissions --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - ADMITTIME (DATETIME)\n",
      "    - DISCHTIME (DATETIME)\n",
      "    - DEATHTIME (DATETIME)\n",
      "    - ADMISSION_TYPE (STRING)\n",
      "    - ADMISSION_LOCATION (STRING)\n",
      "    - DISCHARGE_LOCATION (STRING)\n",
      "    - INSURANCE (STRING)\n",
      "    - LANGUAGE (STRING)\n",
      "    - RELIGION (STRING)\n",
      "    - MARITAL_STATUS (STRING)\n",
      "    - ETHNICITY (STRING)\n",
      "    - EDREGTIME (DATETIME)\n",
      "    - EDOUTTIME (DATETIME)\n",
      "    - DIAGNOSIS (STRING)\n",
      "    - HOSPITAL_EXPIRE_FLAG (INT64)\n",
      "    - HAS_CHARTEVENTS_DATA (INT64)\n",
      "\n",
      "-- Table: callout --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - SUBMIT_WARDID (INT64)\n",
      "    - SUBMIT_CAREUNIT (STRING)\n",
      "    - CURR_WARDID (INT64)\n",
      "    - CURR_CAREUNIT (STRING)\n",
      "    - CALLOUT_WARDID (INT64)\n",
      "    - CALLOUT_SERVICE (STRING)\n",
      "    - REQUEST_TELE (INT64)\n",
      "    - REQUEST_RESP (INT64)\n",
      "    - REQUEST_CDIFF (INT64)\n",
      "    - REQUEST_MRSA (INT64)\n",
      "    - REQUEST_VRE (INT64)\n",
      "    - CALLOUT_STATUS (STRING)\n",
      "    - CALLOUT_OUTCOME (STRING)\n",
      "    - DISCHARGE_WARDID (INT64)\n",
      "    - ACKNOWLEDGE_STATUS (STRING)\n",
      "    - CREATETIME (DATETIME)\n",
      "    - UPDATETIME (DATETIME)\n",
      "    - ACKNOWLEDGETIME (DATETIME)\n",
      "    - OUTCOMETIME (DATETIME)\n",
      "    - FIRSTRESERVATIONTIME (DATETIME)\n",
      "    - CURRENTRESERVATIONTIME (DATETIME)\n",
      "\n",
      "-- Table: caregivers --\n",
      "    - ROW_ID (INT64)\n",
      "    - CGID (INT64)\n",
      "    - LABEL (STRING)\n",
      "    - DESCRIPTION (STRING)\n",
      "\n",
      "-- Table: chartevents --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - ICUSTAY_ID (INT64)\n",
      "    - ITEMID (INT64)\n",
      "    - CHARTTIME (DATETIME)\n",
      "    - STORETIME (DATETIME)\n",
      "    - CGID (INT64)\n",
      "    - VALUE (STRING)\n",
      "    - VALUENUM (FLOAT64)\n",
      "    - VALUEUOM (STRING)\n",
      "    - WARNING (INT64)\n",
      "    - ERROR (INT64)\n",
      "    - RESULTSTATUS (STRING)\n",
      "    - STOPPED (STRING)\n",
      "\n",
      "-- Table: cptevents --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - COSTCENTER (STRING)\n",
      "    - CHARTDATE (DATETIME)\n",
      "    - CPT_CD (STRING)\n",
      "    - CPT_NUMBER (INT64)\n",
      "    - CPT_SUFFIX (BOOL)\n",
      "    - TICKET_ID_SEQ (INT64)\n",
      "    - SECTIONHEADER (STRING)\n",
      "    - SUBSECTIONHEADER (STRING)\n",
      "    - DESCRIPTION (STRING)\n",
      "\n",
      "-- Table: d_cpt --\n",
      "    - ROW_ID (INT64)\n",
      "    - CATEGORY (INT64)\n",
      "    - SECTIONRANGE (STRING)\n",
      "    - SECTIONHEADER (STRING)\n",
      "    - SUBSECTIONRANGE (STRING)\n",
      "    - SUBSECTIONHEADER (STRING)\n",
      "    - CODESUFFIX (BOOL)\n",
      "    - MINCODEINSUBSECTION (INT64)\n",
      "    - MAXCODEINSUBSECTION (INT64)\n",
      "\n",
      "-- Table: d_icd_diagnoses --\n",
      "    - ROW_ID (INT64)\n",
      "    - ICD9_CODE (STRING)\n",
      "    - SHORT_TITLE (STRING)\n",
      "    - LONG_TITLE (STRING)\n",
      "\n",
      "-- Table: d_icd_procedures --\n",
      "    - row_id (INT64)\n",
      "    - icd9_code (STRING)\n",
      "    - short_title (STRING)\n",
      "    - long_title (STRING)\n",
      "\n",
      "-- Table: d_items --\n",
      "    - ROW_ID (INT64)\n",
      "    - ITEMID (INT64)\n",
      "    - LABEL (STRING)\n",
      "    - ABBREVIATION (STRING)\n",
      "    - DBSOURCE (STRING)\n",
      "    - LINKSTO (STRING)\n",
      "    - CATEGORY (STRING)\n",
      "    - UNITNAME (STRING)\n",
      "    - PARAM_TYPE (STRING)\n",
      "    - CONCEPTID (STRING)\n",
      "\n",
      "-- Table: d_labitems --\n",
      "    - ROW_ID (INT64)\n",
      "    - ITEMID (INT64)\n",
      "    - LABEL (STRING)\n",
      "    - FLUID (STRING)\n",
      "    - CATEGORY (STRING)\n",
      "    - LOINC_CODE (STRING)\n",
      "\n",
      "-- Table: datetimeevents --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - ICUSTAY_ID (INT64)\n",
      "    - ITEMID (INT64)\n",
      "    - CHARTTIME (DATETIME)\n",
      "    - STORETIME (DATETIME)\n",
      "    - CGID (INT64)\n",
      "    - VALUE (DATETIME)\n",
      "    - VALUEUOM (STRING)\n",
      "    - WARNING (INT64)\n",
      "    - ERROR (INT64)\n",
      "    - RESULTSTATUS (STRING)\n",
      "    - STOPPED (STRING)\n",
      "\n",
      "-- Table: diagnoses_icd --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - SEQ_NUM (INT64)\n",
      "    - ICD9_CODE (STRING)\n",
      "\n",
      "-- Table: drgcodes --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - DRG_TYPE (STRING)\n",
      "    - DRG_CODE (STRING)\n",
      "    - DESCRIPTION (STRING)\n",
      "    - DRG_SEVERITY (INT64)\n",
      "    - DRG_MORTALITY (INT64)\n",
      "\n",
      "-- Table: icustays --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - ICUSTAY_ID (INT64)\n",
      "    - DBSOURCE (STRING)\n",
      "    - FIRST_CAREUNIT (STRING)\n",
      "    - LAST_CAREUNIT (STRING)\n",
      "    - FIRST_WARDID (INT64)\n",
      "    - LAST_WARDID (INT64)\n",
      "    - INTIME (DATETIME)\n",
      "    - OUTTIME (DATETIME)\n",
      "    - LOS (FLOAT64)\n",
      "\n",
      "-- Table: inputevents_cv --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - ICUSTAY_ID (INT64)\n",
      "    - CHARTTIME (DATETIME)\n",
      "    - ITEMID (INT64)\n",
      "    - AMOUNT (FLOAT64)\n",
      "    - AMOUNTUOM (STRING)\n",
      "    - RATE (FLOAT64)\n",
      "    - RATEUOM (STRING)\n",
      "    - STORETIME (DATETIME)\n",
      "    - CGID (INT64)\n",
      "    - ORDERID (INT64)\n",
      "    - LINKORDERID (INT64)\n",
      "    - STOPPED (STRING)\n",
      "    - NEWBOTTLE (INT64)\n",
      "    - ORIGINALAMOUNT (FLOAT64)\n",
      "    - ORIGINALAMOUNTUOM (STRING)\n",
      "    - ORIGINALROUTE (STRING)\n",
      "    - ORIGINALRATE (FLOAT64)\n",
      "    - ORIGINALRATEUOM (STRING)\n",
      "    - ORIGINALSITE (STRING)\n",
      "\n",
      "-- Table: inputevents_mv --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - ICUSTAY_ID (INT64)\n",
      "    - STARTTIME (DATETIME)\n",
      "    - ENDTIME (DATETIME)\n",
      "    - ITEMID (INT64)\n",
      "    - AMOUNT (FLOAT64)\n",
      "    - AMOUNTUOM (STRING)\n",
      "    - RATE (FLOAT64)\n",
      "    - RATEUOM (STRING)\n",
      "    - STORETIME (DATETIME)\n",
      "    - CGID (INT64)\n",
      "    - ORDERID (INT64)\n",
      "    - LINKORDERID (INT64)\n",
      "    - ORDERCATEGORYNAME (STRING)\n",
      "    - SECONDARYORDERCATEGORYNAME (STRING)\n",
      "    - ORDERCOMPONENTTYPEDESCRIPTION (STRING)\n",
      "    - ORDERCATEGORYDESCRIPTION (STRING)\n",
      "    - PATIENTWEIGHT (FLOAT64)\n",
      "    - TOTALAMOUNT (FLOAT64)\n",
      "    - TOTALAMOUNTUOM (STRING)\n",
      "    - ISOPENBAG (INT64)\n",
      "    - CONTINUEINNEXTDEPT (INT64)\n",
      "    - CANCELREASON (INT64)\n",
      "    - STATUSDESCRIPTION (STRING)\n",
      "    - COMMENTS_EDITEDBY (STRING)\n",
      "    - COMMENTS_CANCELEDBY (STRING)\n",
      "    - COMMENTS_DATE (DATETIME)\n",
      "    - ORIGINALAMOUNT (FLOAT64)\n",
      "    - ORIGINALRATE (FLOAT64)\n",
      "\n",
      "-- Table: labevents --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - ITEMID (INT64)\n",
      "    - CHARTTIME (DATETIME)\n",
      "    - VALUE (STRING)\n",
      "    - VALUENUM (FLOAT64)\n",
      "    - VALUEUOM (STRING)\n",
      "    - FLAG (STRING)\n",
      "\n",
      "-- Table: microbiologyevents --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - CHARTDATE (DATETIME)\n",
      "    - CHARTTIME (DATETIME)\n",
      "    - SPEC_ITEMID (INT64)\n",
      "    - SPEC_TYPE_DESC (STRING)\n",
      "    - ORG_ITEMID (INT64)\n",
      "    - ORG_NAME (STRING)\n",
      "    - ISOLATE_NUM (INT64)\n",
      "    - AB_ITEMID (INT64)\n",
      "    - AB_NAME (STRING)\n",
      "    - DILUTION_TEXT (STRING)\n",
      "    - DILUTION_COMPARISON (STRING)\n",
      "    - DILUTION_VALUE (INT64)\n",
      "    - INTERPRETATION (STRING)\n",
      "\n",
      "-- Table: outputevents --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - ICUSTAY_ID (INT64)\n",
      "    - CHARTTIME (DATETIME)\n",
      "    - ITEMID (INT64)\n",
      "    - VALUE (FLOAT64)\n",
      "    - VALUEUOM (STRING)\n",
      "    - STORETIME (DATETIME)\n",
      "    - CGID (INT64)\n",
      "    - STOPPED (STRING)\n",
      "    - NEWBOTTLE (STRING)\n",
      "    - ISERROR (STRING)\n",
      "\n",
      "-- Table: patients --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - GENDER (STRING)\n",
      "    - DOB (DATETIME)\n",
      "    - DOD (DATETIME)\n",
      "    - DOD_HOSP (DATETIME)\n",
      "    - DOD_SSN (DATETIME)\n",
      "    - EXPIRE_FLAG (INT64)\n",
      "\n",
      "-- Table: prescriptions --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - ICUSTAY_ID (INT64)\n",
      "    - STARTDATE (DATETIME)\n",
      "    - ENDDATE (DATETIME)\n",
      "    - DRUG_TYPE (STRING)\n",
      "    - DRUG (STRING)\n",
      "    - DRUG_NAME_POE (STRING)\n",
      "    - DRUG_NAME_GENERIC (STRING)\n",
      "    - FORMULARY_DRUG_CD (STRING)\n",
      "    - GSN (STRING)\n",
      "    - NDC (INT64)\n",
      "    - PROD_STRENGTH (STRING)\n",
      "    - DOSE_VAL_RX (STRING)\n",
      "    - DOSE_UNIT_RX (STRING)\n",
      "    - FORM_VAL_DISP (STRING)\n",
      "    - FORM_UNIT_DISP (STRING)\n",
      "    - ROUTE (STRING)\n",
      "\n",
      "-- Table: procedureevents_mv --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - ICUSTAY_ID (INT64)\n",
      "    - STARTTIME (DATETIME)\n",
      "    - ENDTIME (DATETIME)\n",
      "    - ITEMID (INT64)\n",
      "    - VALUE (FLOAT64)\n",
      "    - VALUEUOM (STRING)\n",
      "    - LOCATION (STRING)\n",
      "    - LOCATIONCATEGORY (STRING)\n",
      "    - STORETIME (DATETIME)\n",
      "    - CGID (INT64)\n",
      "    - ORDERID (INT64)\n",
      "    - LINKORDERID (INT64)\n",
      "    - ORDERCATEGORYNAME (STRING)\n",
      "    - SECONDARYORDERCATEGORYNAME (STRING)\n",
      "    - ORDERCATEGORYDESCRIPTION (STRING)\n",
      "    - ISOPENBAG (INT64)\n",
      "    - CONTINUEINNEXTDEPT (INT64)\n",
      "    - CANCELREASON (INT64)\n",
      "    - STATUSDESCRIPTION (STRING)\n",
      "    - COMMENTS_EDITEDBY (STRING)\n",
      "    - COMMENTS_CANCELEDBY (STRING)\n",
      "    - COMMENTS_DATE (DATETIME)\n",
      "\n",
      "-- Table: procedures_icd --\n",
      "    - row_id (INT64)\n",
      "    - subject_id (INT64)\n",
      "    - hadm_id (INT64)\n",
      "    - seq_num (INT64)\n",
      "    - icd9_code (STRING)\n",
      "\n",
      "-- Table: services --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - TRANSFERTIME (DATETIME)\n",
      "    - PREV_SERVICE (STRING)\n",
      "    - CURR_SERVICE (STRING)\n",
      "\n",
      "-- Table: transfers --\n",
      "    - ROW_ID (INT64)\n",
      "    - SUBJECT_ID (INT64)\n",
      "    - HADM_ID (INT64)\n",
      "    - ICUSTAY_ID (INT64)\n",
      "    - DBSOURCE (STRING)\n",
      "    - EVENTTYPE (STRING)\n",
      "    - PREV_CAREUNIT (STRING)\n",
      "    - CURR_CAREUNIT (STRING)\n",
      "    - PREV_WARDID (INT64)\n",
      "    - CURR_WARDID (INT64)\n",
      "    - INTIME (DATETIME)\n",
      "    - OUTTIME (DATETIME)\n",
      "    - LOS (FLOAT64)\n",
      "\n",
      "--- End of Schema ---\n"
     ]
    }
   ],
   "source": [
    "# The project ID for the public MIMIC-III dataset\n",
    "schema_project_id = \"physionet-data\"\n",
    "schema_dataset_id = \"mimiciii_clinical\"\n",
    "\n",
    "# Query the INFORMATION_SCHEMA to get table and column details\n",
    "schema_query = f\"\"\"\n",
    "    SELECT\n",
    "        table_name,\n",
    "        column_name,\n",
    "        data_type\n",
    "    FROM `{schema_project_id}.{schema_dataset_id}.INFORMATION_SCHEMA.COLUMNS`\n",
    "    ORDER BY\n",
    "        table_name,\n",
    "        ordinal_position;\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "schema_df = client.query(schema_query).to_dataframe()\n",
    "\n",
    "# Process and print the schema\n",
    "if schema_df.empty:\n",
    "    print(\"Could not retrieve schema. The dataset might be empty or inaccessible.\")\n",
    "else:\n",
    "    print(\"--- MIMIC-III Clinical Dataset Schema ---\")\n",
    "    current_table = \"\"\n",
    "    for index, row in schema_df.iterrows():\n",
    "        if row['table_name'] != current_table:\n",
    "            current_table = row['table_name']\n",
    "            print(f\"\\n-- Table: {current_table} --\")\n",
    "        print(f\"    - {row['column_name']} ({row['data_type']})\")\n",
    "    print(\"\\n--- End of Schema ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Framework\n",
    "\n",
    "This section outlines the development of our prediction models, adhering to the project guidelines. We will implement a full pipeline, from data extraction to model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project constants based on the guidelines\n",
    "DATA_COLLECTION_HOURS = 48\n",
    "PREDICTION_GAP_HOURS = 6\n",
    "MIN_HOSPITALIZATION_HOURS = DATA_COLLECTION_HOURS + PREDICTION_GAP_HOURS  # 54 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Extraction\n",
    "\n",
    "**Goal:** Extract and preprocess the required data modalities for our cohort.\n",
    "\n",
    "According to the guidelines, we need to extract:\n",
    "- Demographic features (e.g., age, gender)\n",
    "- Vital signs\n",
    "- Laboratory test results (from `Labevents`)\n",
    "- At least two other data modalities (e.g., prescriptions, microbiology events)\n",
    "\n",
    "We will focus on the **first hospital admission** for each patient and only include patients with at least **54 hours of hospitalization**. All features will be extracted from the **first 48 hours** of the admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 32513 subject IDs from initial_cohort.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the initial cohort from the provided CSV file\n",
    "try:\n",
    "    cohort_subject_ids_df = pd.read_csv('../data/initial_cohort.csv')\n",
    "    subject_ids = cohort_subject_ids_df['subject_id'].tolist()\n",
    "    print(f\"Successfully loaded {len(subject_ids)} subject IDs from initial_cohort.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'data/initial_cohort.csv' not found.\")\n",
    "    print(\"Please ensure the file exists and the path is correct.\")\n",
    "    subject_ids = [] # Set to empty list to prevent query errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pickh\\pythonProject\\MLH\\.venv\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:1957: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28552 patients from the CSV who meet the criteria.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>deathtime</th>\n",
       "      <th>gender</th>\n",
       "      <th>dob</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>insurance</th>\n",
       "      <th>rn</th>\n",
       "      <th>length_of_stay_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4328</td>\n",
       "      <td>188261</td>\n",
       "      <td>2138-04-30 21:39:00</td>\n",
       "      <td>2138-05-13 15:40:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>F</td>\n",
       "      <td>2078-07-03</td>\n",
       "      <td>HISPANIC OR LATINO</td>\n",
       "      <td>Government</td>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4871</td>\n",
       "      <td>137235</td>\n",
       "      <td>2109-08-22 07:15:00</td>\n",
       "      <td>2109-09-11 20:30:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>M</td>\n",
       "      <td>2052-12-05</td>\n",
       "      <td>BLACK/AFRICAN AMERICAN</td>\n",
       "      <td>Government</td>\n",
       "      <td>1</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7503</td>\n",
       "      <td>152738</td>\n",
       "      <td>2182-05-16 03:29:00</td>\n",
       "      <td>2182-05-30 18:30:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>F</td>\n",
       "      <td>2182-05-16</td>\n",
       "      <td>UNKNOWN/NOT SPECIFIED</td>\n",
       "      <td>Government</td>\n",
       "      <td>1</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18939</td>\n",
       "      <td>189797</td>\n",
       "      <td>2193-05-11 16:00:00</td>\n",
       "      <td>2193-05-21 16:16:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>F</td>\n",
       "      <td>2193-05-11</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>Government</td>\n",
       "      <td>1</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50201</td>\n",
       "      <td>108169</td>\n",
       "      <td>2106-11-26 07:15:00</td>\n",
       "      <td>2106-11-30 12:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>F</td>\n",
       "      <td>2066-01-30</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>Government</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hadm_id           admittime           dischtime deathtime  \\n",
       "0        4328   188261 2138-04-30 21:39:00 2138-05-13 15:40:00       NaT   \\n",
       "1        4871   137235 2109-08-22 07:15:00 2109-09-11 20:30:00       NaT   \\n",
       "2        7503   152738 2182-05-16 03:29:00 2182-05-30 18:30:00       NaT   \\n",
       "3       18939   189797 2193-05-11 16:00:00 2193-05-21 16:16:00       NaT   \\n",
       "4       50201   108169 2106-11-26 07:15:00 2106-11-30 12:00:00       NaT   \\n",
       "\\n",
       "  gender        dob               ethnicity   insurance  rn  \\n",
       "0      F 2078-07-03      HISPANIC OR LATINO  Government   1   \\n",
       "1      M 2052-12-05  BLACK/AFRICAN AMERICAN  Government   1   \\n",
       "2      F 2182-05-16   UNKNOWN/NOT SPECIFIED  Government   1   \\n",
       "3      F 2193-05-11                   OTHER  Government   1   \\n",
       "4      F 2066-01-30                   WHITE  Government   1   \\n",
       "\\n",
       "   length_of_stay_hours  \n",
       "0                   306  \n",
       "1                   493  \n",
       "2                   351  \n",
       "3                   240  \n",
       "4                   101  "
     ]
    }
   ],
   "source": [
    "# This query now fetches data only for the patients specified in the CSV file.\n",
    "# It still identifies their first admission and ensures their stay is long enough.\n",
    "\n",
    "initial_cohort_query = '''\n",
    "-- This query extracts the first admission for a specified list of patients,\n",
    "-- ensuring their hospital stay meets a minimum duration.\n",
    "\n",
    "-- Step 1: Filter admissions to only include the patients specified in the cohort\n",
    "WITH admissions_for_cohort AS (\n",
    "    SELECT\n",
    "        p.subject_id,\n",
    "        a.hadm_id,\n",
    "        a.admittime,\n",
    "        a.dischtime,\n",
    "        a.deathtime,\n",
    "        p.gender,\n",
    "        p.dob,\n",
    "        a.ethnicity,\n",
    "        a.insurance\n",
    "    FROM `physionet-data.mimiciii_clinical.patients` p\n",
    "    INNER JOIN `physionet-data.mimiciii_clinical.admissions` a ON p.subject_id = a.subject_id\n",
    "    WHERE\n",
    "        -- Only include patients from our initial cohort list\n",
    "        p.subject_id IN UNNEST(@subject_ids)\n",
    "        -- And ensure the admission has associated chart events\n",
    "        AND a.has_chartevents_data = 1\n",
    ")\n",
    "\n",
    "-- Step 2: Identify the first admission for each patient\n",
    "first_admissions AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        -- Assign a row number to each admission, ordered by time\n",
    "        ROW_NUMBER() OVER(PARTITION BY subject_id ORDER BY admittime) as rn\n",
    "    FROM admissions_for_cohort\n",
    ")\n",
    "\n",
    "-- Step 3: Calculate length of stay and filter by the minimum required duration\n",
    "final_cohort AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        -- Calculate the duration of the hospital stay in hours\n",
    "        DATETIME_DIFF(dischtime, admittime, HOUR) as length_of_stay_hours\n",
    "    FROM first_admissions\n",
    "    WHERE\n",
    "        -- We only want the very first admission\n",
    "        rn = 1\n",
    ")\n",
    "\n",
    "-- Final Selection: Select all columns from the processed and filtered cohort\n",
    "SELECT *\n",
    "FROM final_cohort\n",
    "WHERE\n",
    "    -- Ensure the hospital stay is long enough for our analysis\n",
    "    length_of_stay_hours >= @min_hospitalization_hours\n",
    "'''\n",
    "\n",
    "query_params = [\n",
    "    bigquery.ArrayQueryParameter(\"subject_ids\", \"INT64\", subject_ids),\n",
    "    bigquery.ScalarQueryParameter(\"min_hospitalization_hours\", \"INT64\", MIN_HOSPITALIZATION_HOURS),\n",
    "]\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    query_parameters=query_params\n",
    ")\n",
    "\n",
    "# Only run the query if subject_ids were loaded successfully\n",
    "if subject_ids:\n",
    "    initial_cohort_df = client.query(initial_cohort_query, job_config=job_config).to_dataframe()\n",
    "    print(f\"Found {len(initial_cohort_df)} patients from the CSV who meet the criteria.\")\n",
    "    display(initial_cohort_df.head())\n",
    "else:\n",
    "    print(\"Query skipped because no subject IDs were loaded.\")\n",
    "    initial_cohort_df = pd.DataFrame() # Create an empty dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Definition\n",
    "\n",
    "**Goal:** Annotate the three clinical targets for our patient cohort.\n",
    "\n",
    "The targets are:\n",
    "1.  **Mortality:** Death during hospitalization or within 30 days after discharge.\n",
    "2.  **Prolonged Stay:** Hospitalization length > 7 days.\n",
    "3.  **Hospital Readmission:** Readmission within 30 days after discharge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pickh\\pythonProject\\MLH\\.venv\\Lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:489: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\\n",
      "c:\\Users\\pickh\\pythonProject\\MLH\\.venv\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:1957: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\\n"
     ]
    }
   ],
   "source": [
    "# Query to find all subsequent admissions for the patients in our cohort\n",
    "readmission_query = \"\"\"\n",
    "WITH cohort_admissions AS (\n",
    "    -- This is our initial cohort of first admissions\n",
    "    SELECT \n",
    "        subject_id, \n",
    "        hadm_id, \n",
    "        dischtime\n",
    "    FROM initial_cohort_df\n",
    ")\n",
    "SELECT \n",
    "    a.subject_id,\n",
    "    -- For each admission in our cohort, find the next admission time\n",
    "    LEAD(a.admittime, 1) OVER (PARTITION BY a.subject_id ORDER BY a.admittime) as next_admittime\n",
    "FROM `physionet-data.mimiciii_clinical.admissions` a\n",
    "INNER JOIN cohort_admissions ca ON a.subject_id = ca.subject_id AND a.hadm_id = ca.hadm_id\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "if not initial_cohort_df.empty:\n",
    "    # Create a temporary table from the initial_cohort_df to use in the query\n",
    "    client.create_dataset('temp_dataset', exists_ok=True)\n",
    "    table_ref = client.dataset('temp_dataset').table('initial_cohort_df')\n",
    "    job = client.load_table_from_dataframe(initial_cohort_df[['subject_id', 'hadm_id', 'dischtime']], table_ref)\n",
    "    job.result() # Wait for the table to be created\n",
    "    \n",
    "    # Now run the query using the temp table\n",
    "    readmission_df = client.query(readmission_query.replace('initial_cohort_df', '`temp_dataset.initial_cohort_df`')).to_dataframe()\n",
    "    \n",
    "    # Merge the readmission time back into our main cohort dataframe\n",
    "    initial_cohort_df = pd.merge(initial_cohort_df, readmission_df, on='subject_id', how='left')\n",
    "    \n",
    "    # Clean up the temp table\n",
    "    client.delete_table(table_ref, not_found_ok=True)\n",
    "else:\n",
    "    initial_cohort_df['next_admittime'] = pd.NaT # Add empty column if cohort is empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>target_mortality</th>\n",
       "      <th>target_prolonged_stay</th>\n",
       "      <th>target_readmission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4328</td>\n",
       "      <td>188261</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4871</td>\n",
       "      <td>137235</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7503</td>\n",
       "      <td>152738</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18939</td>\n",
       "      <td>189797</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50201</td>\n",
       "      <td>108169</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hadm_id  target_mortality  target_prolonged_stay  \\n",
       "0        4328   188261             False                   True   \\n",
       "1        4871   137235             False                   True   \\n",
       "2        7503   152738             False                   True   \\n",
       "3       18939   189797             False                   True   \\n",
       "4       50201   108169             False                  False   \\n",
       "\\n",
       "   target_readmission  \n",
       "0               False  \n",
       "1               False  \n",
       "2               False  \n",
       "3               False  \n",
       "4               False  "
     ]
    }
   ],
   "source": [
    "def define_targets(df):\n",
    "    \"\"\"Defines the three targets based on the project guidelines.\"\"\"\n",
    "    # Target 1: Mortality\n",
    "    # Death during hospital stay or within 30 days of discharge\n",
    "    df['mortality_in_hospital'] = df['deathtime'].notna() & (df['deathtime'] <= df['dischtime'])\n",
    "    df['mortality_after_discharge'] = df['deathtime'].notna() & (df['deathtime'] > df['dischtime']) & (df['deathtime'] <= df['dischtime'] + pd.to_timedelta('30 days'))\n",
    "    df['target_mortality'] = df['mortality_in_hospital'] | df['mortality_after_discharge']\n",
    "\n",
    "    # Target 2: Prolonged Stay\n",
    "    # Length of stay > 7 days\n",
    "    df['target_prolonged_stay'] = df['length_of_stay_hours'] > 7 * 24\n",
    "\n",
    "    # Target 3: Hospital Readmission\n",
    "    # Readmission within 30 days of discharge\n",
    "    if 'next_admittime' in df.columns:\n",
    "        df['readmission_window_end'] = df['dischtime'] + pd.to_timedelta('30 days')\n",
    "        df['target_readmission'] = df['next_admittime'].notna() & (df['next_admittime'] <= df['readmission_window_end'])\n",
    "        df = df.drop(columns=['next_admittime', 'readmission_window_end'])\n",
    "    else:\n",
    "        df['target_readmission'] = False\n",
    "    \n",
    "    # Clean up helper columns\n",
    "    df = df.drop(columns=['mortality_in_hospital', 'mortality_after_discharge'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "cohort_with_targets_df = define_targets(initial_cohort_df.copy())\n",
    "cohort_with_targets_df[['subject_id', 'hadm_id', 'target_mortality', 'target_prolonged_stay', 'target_readmission']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Goal:** Understand the characteristics of our cohort and inform subsequent data processing and modeling decisions.\n",
    "\n",
    "In this section, we will:\n",
    "1. Examine the distribution of demographic features\n",
    "2. Analyze the prevalence of our target outcomes\n",
    "3. Explore relationships between features and targets\n",
    "4. Identify potential issues that need to be addressed in preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set the style for our visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# Check if our cohort data is available\n",
    "if not cohort_with_targets_df.empty:\n",
    "    print(f\"Cohort size: {len(cohort_with_targets_df)} patients\")\n",
    "else:\n",
    "    print(\"No cohort data available for analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1 Demographic Analysis\n",
    "\n",
    "# Function to calculate age from DOB and admission time\n",
    "def calculate_age(row):\n",
    "    \"\"\"Calculate age at admission in years.\"\"\"\n",
    "    if pd.isnull(row['dob']) or pd.isnull(row['admittime']):\n",
    "        return np.nan\n",
    "    \n",
    "    # MIMIC-III shifts dates for deidentification, so some ages might be negative\n",
    "    # We'll take the absolute value to correct for this\n",
    "    age_days = (row['admittime'] - row['dob']).days\n",
    "    age_years = abs(age_days) / 365.25\n",
    "    \n",
    "    # Cap ages at 90 for privacy in MIMIC\n",
    "    return min(age_years, 90)\n",
    "\n",
    "if not cohort_with_targets_df.empty:\n",
    "    # Add age column\n",
    "    cohort_with_targets_df['age_at_admission'] = cohort_with_targets_df.apply(calculate_age, axis=1)\n",
    "    \n",
    "    # Demographic distributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Age distribution\n",
    "    sns.histplot(cohort_with_targets_df['age_at_admission'].dropna(), kde=True, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Age Distribution')\n",
    "    axes[0, 0].set_xlabel('Age (years)')\n",
    "    \n",
    "    # Gender distribution\n",
    "    gender_counts = cohort_with_targets_df['gender'].value_counts()\n",
    "    axes[0, 1].pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 1].set_title('Gender Distribution')\n",
    "    \n",
    "    # Ethnicity distribution (top 5)\n",
    "    ethnicity_counts = cohort_with_targets_df['ethnicity'].value_counts().head(5)\n",
    "    sns.barplot(x=ethnicity_counts.index, y=ethnicity_counts.values, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Top 5 Ethnicities')\n",
    "    axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Insurance distribution\n",
    "    insurance_counts = cohort_with_targets_df['insurance'].value_counts()\n",
    "    sns.barplot(x=insurance_counts.index, y=insurance_counts.values, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Insurance Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics for age\n",
    "    print(\"\\nAge Statistics:\")\n",
    "    print(cohort_with_targets_df['age_at_admission'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2 Target Variable Analysis\n",
    "\n",
    "if not cohort_with_targets_df.empty:\n",
    "    # Calculate target prevalence\n",
    "    target_columns = ['target_mortality', 'target_prolonged_stay', 'target_readmission']\n",
    "    target_prevalence = cohort_with_targets_df[target_columns].mean()\n",
    "    \n",
    "    # Plot target prevalence\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=target_prevalence.index, y=target_prevalence.values)\n",
    "    plt.title('Target Prevalence')\n",
    "    plt.ylabel('Proportion of Patients')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, v in enumerate(target_prevalence.values):\n",
    "        plt.text(i, v + 0.02, f\"{v:.1%}\", ha='center')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Check for correlations between targets\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cohort_with_targets_df[target_columns].corr(), \n",
    "                annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Between Target Variables')\n",
    "    plt.show()\n",
    "    \n",
    "    # Examine target distribution by demographics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Gender vs targets\n",
    "    for i, target in enumerate(target_columns):\n",
    "        gender_target = pd.crosstab(\n",
    "            cohort_with_targets_df['gender'], \n",
    "            cohort_with_targets_df[target], \n",
    "            normalize='index'\n",
    "        )\n",
    "        gender_target[True].plot(kind='bar', ax=axes[i], color='skyblue')\n",
    "        axes[i].set_title(f'{target.replace(\"target_\", \"\").replace(\"_\", \" \").title()} by Gender')\n",
    "        axes[i].set_ylabel('Proportion')\n",
    "        axes[i].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Age group vs targets\n",
    "    cohort_with_targets_df['age_group'] = pd.cut(\n",
    "        cohort_with_targets_df['age_at_admission'], \n",
    "        bins=[0, 18, 40, 65, 90], \n",
    "        labels=['0-18', '19-40', '41-65', '65+']\n",
    "    )\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for i, target in enumerate(target_columns):\n",
    "        age_target = pd.crosstab(\n",
    "            cohort_with_targets_df['age_group'], \n",
    "            cohort_with_targets_df[target], \n",
    "            normalize='index'\n",
    "        )\n",
    "        age_target[True].plot(kind='bar', ax=axes[i], color='lightgreen')\n",
    "        axes[i].set_title(f'{target.replace(\"target_\", \"\").replace(\"_\", \" \").title()} by Age Group')\n",
    "        axes[i].set_ylabel('Proportion')\n",
    "        axes[i].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.3 Hospital Stay Analysis\n",
    "\n",
    "if not cohort_with_targets_df.empty:\n",
    "    # Convert length of stay from hours to days for better interpretability\n",
    "    cohort_with_targets_df['length_of_stay_days'] = cohort_with_targets_df['length_of_stay_hours'] / 24\n",
    "    \n",
    "    # Length of stay distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(cohort_with_targets_df['length_of_stay_days'], bins=30, kde=True)\n",
    "    plt.axvline(x=7, color='red', linestyle='--', label='7 days (prolonged stay threshold)')\n",
    "    plt.title('Length of Stay Distribution')\n",
    "    plt.xlabel('Length of Stay (days)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Length of stay statistics\n",
    "    print(\"Length of Stay Statistics (days):\")\n",
    "    print(cohort_with_targets_df['length_of_stay_days'].describe())\n",
    "    \n",
    "    # Length of stay by target outcomes\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    target_columns = ['target_mortality', 'target_prolonged_stay', 'target_readmission']\n",
    "    target_labels = ['Mortality', 'Prolonged Stay', 'Readmission']\n",
    "    \n",
    "    for i, (target, label) in enumerate(zip(target_columns, target_labels)):\n",
    "        sns.boxplot(x=cohort_with_targets_df[target], y=cohort_with_targets_df['length_of_stay_days'], ax=axes[i])\n",
    "        axes[i].set_title(f'Length of Stay by {label}')\n",
    "        axes[i].set_xlabel(label)\n",
    "        axes[i].set_ylabel('Length of Stay (days)')\n",
    "        # Set x-tick labels\n",
    "        axes[i].set_xticklabels(['No', 'Yes'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.4 Missing Data Analysis\n",
    "\n",
    "if not cohort_with_targets_df.empty:\n",
    "    # Calculate missing data percentages\n",
    "    missing_data = cohort_with_targets_df.isnull().sum() / len(cohort_with_targets_df) * 100\n",
    "    missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if not missing_data.empty:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x=missing_data.index, y=missing_data.values)\n",
    "        plt.title('Missing Data by Column')\n",
    "        plt.xlabel('Columns')\n",
    "        plt.ylabel('Percentage Missing')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add percentage labels on bars\n",
    "        for i, v in enumerate(missing_data.values):\n",
    "            plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No missing data found in the current columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.5 EDA Summary and Implications\n",
    "\n",
    "# This cell will be used to summarize key findings from the EDA and their implications for:\n",
    "# 1. Data partitioning strategy\n",
    "# 2. Feature engineering decisions\n",
    "# 3. Preprocessing requirements\n",
    "# 4. Modeling considerations\n",
    "\n",
    "if not cohort_with_targets_df.empty:\n",
    "    # Calculate class imbalance ratios\n",
    "    target_columns = ['target_mortality', 'target_prolonged_stay', 'target_readmission']\n",
    "    imbalance_ratios = {}\n",
    "    \n",
    "    for target in target_columns:\n",
    "        positive_count = cohort_with_targets_df[target].sum()\n",
    "        negative_count = len(cohort_with_targets_df) - positive_count\n",
    "        ratio = negative_count / positive_count if positive_count > 0 else float('inf')\n",
    "        imbalance_ratios[target] = ratio\n",
    "    \n",
    "    print(\"Class Imbalance Analysis (Negative:Positive ratio):\")\n",
    "    for target, ratio in imbalance_ratios.items():\n",
    "        target_name = target.replace('target_', '').replace('_', ' ').title()\n",
    "        print(f\"- {target_name}: {ratio:.2f}:1\")\n",
    "    \n",
    "    print(\"\\nKey EDA Findings and Implications:\")\n",
    "    print(\"1. Data Partitioning Strategy:\")\n",
    "    print(\"   - Need to ensure balanced distribution of age groups and gender across partitions\")\n",
    "    print(\"   - Should use stratified sampling based on target variables due to class imbalance\")\n",
    "    print(\"   - Consider stratification by key demographic features identified in the analysis\")\n",
    "    \n",
    "    print(\"\\n2. Feature Engineering:\")\n",
    "    print(\"   - Age appears to be an important predictor and should be binned appropriately\")\n",
    "    print(\"   - Consider creating interaction terms between demographics and clinical variables\")\n",
    "    print(\"   - Time-based features from the 48-hour window may reveal important patterns\")\n",
    "    \n",
    "    print(\"\\n3. Preprocessing Requirements:\")\n",
    "    print(\"   - Need strategy for handling missing data identified in the analysis\")\n",
    "    print(\"   - Categorical variables (ethnicity, insurance) require encoding\")\n",
    "    print(\"   - Length of stay distribution is right-skewed and may need transformation\")\n",
    "    \n",
    "    print(\"\\n4. Modeling Considerations:\")\n",
    "    print(\"   - Class imbalance requires appropriate handling (sampling, weighting, or specialized algorithms)\")\n",
    "    print(\"   - Should evaluate models on metrics suitable for imbalanced data (e.g., PR AUC)\")\n",
    "    print(\"   - Consider ensemble methods to improve performance across different demographic groups\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Partition\n",
    "\n",
    "**Goal:** Split the data into training, validation, and test sets.\n",
    "\n",
    "To prevent data leakage, we will split our cohort by `subject_id` before any feature engineering or preprocessing that looks across patients. A standard 60/20/20 split will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training patients: 17130\n",
      "Validation patients: 5711\n",
      "Test patients: 5711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "patient_ids = cohort_with_targets_df['subject_id'].unique()\n",
    "\n",
    "# Split 80/20 for train/test\n",
    "train_val_ids, test_ids = train_test_split(patient_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the 80% into 75/25 for train/validation (results in 60/20 of total)\n",
    "train_ids, val_ids = train_test_split(train_val_ids, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Training patients: {len(train_ids)}\")\n",
    "print(f\"Validation patients: {len(val_ids)}\")\n",
    "print(f\"Test patients: {len(test_ids)}\")\n",
    "\n",
    "# Create dataframes for each set\n",
    "train__df = cohort_with_targets_df[cohort_with_targets_df['subject_id'].isin(train_ids)]\n",
    "val_df = cohort_with_targets_df[cohort_with_targets_df['subject_id'].isin(val_ids)]\n",
    "test_df = cohort_with_targets_df[cohort_with_targets_df['subject_id'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "**Goal:** Clean the data, handle missing values, and perform feature engineering.\n",
    "\n",
    "This step will involve:\n",
    "- Handling missing data (imputation).\n",
    "- Scaling numerical features.\n",
    "- Encoding categorical features.\n",
    "- Creating features from time-series data (e.g., aggregation within the 48-hour window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for preprocessing pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Example: Define feature types (these will be determined after full data extraction)\n",
    "# numeric_features = ['age_at_admission'] \n",
    "# categorical_features = ['gender', 'ethnicity', 'insurance']\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
",
    "#                                ('scaler', StandardScaler())]), numeric_features),\n",
    "#         ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
",
    "#                                ('onehot', OneHotEncoder(handle_unknown='ignore'))]), categorical_features)\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling\n",
    "\n",
    "**Goal:** Train prediction models for each target.\n",
    "\n",
    "We will start with a baseline model (e.g., Logistic Regression) and explore more complex models. The final model must return **calibrated probabilities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Example for one target\n",
    "# model = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced')\n",
    "\n",
    "# The model must be calibrated\n",
    "# calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv=5)\n",
    "\n",
    "# The full pipeline would look like this:\n",
    "# full_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                                 ('classifier', calibrated_model)])\n",
    "\n",
    "# full_pipeline.fit(X_train, y_train) # Where X_train are features and y_train is the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "**Goal:** Evaluate the models' performance according to the project requirements.\n",
    "\n",
    "We will evaluate:\n",
    "- **Classification performance:** ROC and PR curves.\n",
    "- **Calibration performance:** Calibration curves.\n",
    "- **Feature importance:** SHAP analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "# import shap\n",
    "\n",
    "# # Example evaluation on the validation set\n",
    "# y_pred_proba = full_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# print(f\"ROC AUC: {roc_auc_score(y_val, y_pred_proba)}\")\n",
    "# print(f\"AUPRC: {average_precision_score(y_val, y_pred_proba)}\")\n",
    "\n",
    "# # Placeholder for plotting functions\n",
    "# def plot_roc_curve(y_true, y_pred_proba):\n",
    "#     fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "#     plt.plot(fpr, tpr)\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('ROC Curve')\n",
    "#     plt.show()\n",
    "\n",
    "# plot_roc_curve(y_val, y_pred_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}