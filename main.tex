%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{TRIAGE: Time-series Risk Prediction from ICU Data via a GRU Autoencoder with Contrastive Embeddings}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lital Voulichman}{sch}
\icmlauthor{Yair Pickholz Berliner
}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlkeywords{Machine Learning, Healthcare ML, ICU, Time-series, Contrastive Learning}

\vskip 0.3in
]

% \begin{abstract}
% Accurate early prediction of Intensive Care Unit (ICU) outcomes is essential for guiding interventions and allocating critical resources. Prior studies have shown that risk stratification within the first days of hospitalization enables clinicians to intervene proactively and improve trajectories \cite{deasy2020dynamic}. Deep learning models trained on electronic health record (EHR) time-series have demonstrated reasonable accuracy for predicting mortality, prolonged stay, and readmission \cite{shukla2020integrating}\cite{ruan2025evidence}, yet many approaches struggle to jointly model multiple outcomes, capture temporal dynamics, and provide interpretable risk estimates.

% We present TRIAGE, a multi-task sequence model that predicts three ICU outcomes: (i) in-hospital or 30-day mortality, (ii) prolonged stay (over 7 days), and (iii) 30-day readmission - using only the first 48 hours of admission data with a 6-hour prediction gap. TRIAGE integrates a GRU-based sequence autoencoder with supervised contrastive embeddings, creating a shared patient embedding space that both improves discriminative performance and can enable comparison of patient trajectories for interpretability. 

% Using multimodal features from MIMIC-III (demographics, vitals, labs, microbiology, and prescriptions), TRIAGE outperforms the GRU baselines. These results demonstrate that embedding-based, calibrated multi-task learning can enhance both performance and clarity in early ICU risk prediction.

% \end{abstract}

\begin{abstract}
Accurate early prediction of Intensive Care Unit (ICU) outcomes is critical for timely interventions and resource allocation. Existing deep learning models show promise but often fail to jointly model multiple outcomes while remaining interpretable.

We introduce TRIAGE, a multi-task model that predicts three ICU outcomes from the first 48 hours of admission: mortality, prolonged stay, and readmission. TRIAGE combines a GRU-based sequence autoencoder with contrastive learning to generate shared patient embeddings, improving discrimination and enabling interpretable patient similarity comparisons.

Using multimodal ICU data, TRIAGE achieves strong performance across tasks, with notable gains for rare outcomes. These results show that embedding-based multi-task learning can enhance both accuracy and interpretability in early ICU risk prediction.
\end{abstract}

\section{Introduction}

Early and accurate risk prediction in the ICU is crucial for improving patient outcomes and prioritizing limited resources \cite{deasy2020dynamic}. Deep learning models, especially those based on RNN and GRU architectures, have advanced outcome prediction from EHR time-series data, outperforming traditional methods on tasks like mortality, prolonged stay, and readmission \cite{shukla2020integrating, ruan2025evidence, deng2022explainable}. Despite these gains, existing models are often limited to single tasks, leverage only part of multimodal ICU datasets, and lack interpretable outputs clinicians can trust \cite{deng2022explainable, zang2021scehr}.

Recent methods introduce multi-task learning and supervised contrastive objectives, allowing models to simultaneously predict multiple outcomes and organize patient representations for better discrimination and explainability \cite{zang2021scehr, ma2024global}. However, few approaches combine these advances with robust calibration and comprehensive multimodal input.

We present TRIAGE, a GRU-based multi-task autoencoder with supervised contrastive learning, designed to jointly predict mortality, prolonged stay, and readmission using all available ICU data from the first 48 hours. TRIAGE yields accurate, calibrated risk predictions and interpretable patient embeddings, enabling case-based reasoning and surpassing standard baselines on MIMIC-III.

\section{Methods}
\subsection{Dataset Characteristics}

Our cohort was constructed from provided subject IDs, comprising 27,636 unique patients from MIMIC-III with 190,963 observations over 48-hour windows. The population had a median age of 65.6 years with 56.6\% male patients. Target prevalences were: 11.7\% mortality, 50.2\% prolonged stay, and 3.9\% readmission, reflecting substantial class imbalance that motivated our specialized sampling and loss weighting strategies.

The feature set included 274 variables across four modalities with varying completeness: vital signs (36 features, 62-72\%), laboratories (33 features, 78-88\%), medications (77 features, high completeness), and microbiology (33 features, selective based on clinical suspicion).

\subsection{Data Pre-processing}

Following project guidelines, only the first hospital admission for each patient was used, with admissions shorter than 54 hours excluded to ensure a full 48-hour observation period plus a 6-hour prediction gap.

For laboratory and vital signs, we filtered measurements against metadata-defined physiological ranges (provided in HW materials) to remove implausible values. Time stamps were aligned to non-overlapping 6-hour bins, providing a balance between temporal resolution and data sparsity: shorter bins would increase missingness and noise, while longer bins obscure clinically relevant fluctuations.

Missing data were imputed by forward-filling within each admission; any remaining missingness was filled with patient-specific baseline values (first-day measurements). Continuous features were standardized via z-score normalization, fitted on the training set only to avoid data leakage.

Finally, all features were padded into fixed-length sequences ($N$ patients $\times$ $T{=}8$ time steps $\times$ $D$ features), with binary masks tracking valid windows. Labels for the three prediction tasks were aligned at the patient level. The resulting dataset was split into training, validation, and test subsets (80/10/10) using grouped patient-level splits to prevent data leakage across admissions.

\subsection{Feature Engineering}

We represented each patient as a multivariate time series over the first 48 hours of admission. Beyond demographics, laboratory tests, and vital signs, we included prescriptions and microbiology as additional modalities. These were chosen because medication patterns reflect treatment intensity and clinical judgment, while microbiology captures suspected sources of infection and their severity.

\textbf{Demographics}: Gender was encoded as binary, and ethnicity was collapsed into five categories (White, Black, Hispanic, Asian, Other) with one-hot encoding.

\textbf{Lab and Vital Dynamics}: To capture patient instability, we engineered lab difference-from-baseline features (each value compared to the patient's first recorded measurement) and vital sign first-order differences between consecutive 6-hour windows. Key vitals included heart rate (72\% completeness), blood pressure measures (62\%), and respiratory parameters.

\textbf{Prescriptions}: Drug names were mapped into curated clinical categories using keyword dictionaries. Common categories included antibiotics (12.8\% usage rate), anticoagulants (11.6\%), sedatives (9.5\%), and insulin (10.3\%). For each 6-hour window, we derived binary indicators for drug administration, distinct drug counts, and normalized doses for high-impact categories.

\textbf{Microbiology}: Specimen sources were grouped into anatomical categories, with blood cultures representing 62.9\% of microbiology events. We added indicators for culture positivity, antibiotic susceptibility, and common organisms, providing signals about infection source and severity.

\subsection{Model}

Firstly, training uses a \textbf{balanced positive-per-task sampler}, which guarantees sufficient positive cases per batch while keeping the remainder representative of the overall population.

Then, inspired by the paper we presented in class, we implemented a \textbf{multitask sequence autoencoder with supervised contrastive learning} (Fig.~\ref{fig:model}). 

The backbone is a GRU autoencoder trained with three complementary objectives: 

\begin{itemize}
    \item \textbf{Reconstruction:} Forces the encoder to retain broad patient-level information about the time series, ensuring the latent representation remains informative.  
    \item \textbf{Outcome classification:} Task-specific linear heads predict mortality, prolonged stay, and readmission. Class imbalance is addressed with weighted binary cross-entropy.  
    \item \textbf{Supervised Contrastive learning:} Projection heads encourage patients with the same outcome label to cluster in latent space, while rare positive cases are emphasized through positive-only anchors and weighting.  
\end{itemize}

These objectives are combined into a weighted total loss. A short \textbf{warm-up phase} trains the encoder with reconstruction and contrastive objectives before introducing classification, stabilizing the latent space. 

Latent embeddings are pooled by concatenating the GRU’s final hidden state, mean, and max across time, capturing both long-term trends and acute peaks.  

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{model_sketch.png}
    \caption{Architecture of the multi-task GRU autoencoder with supervised contrastive learning. 
    The GRU encoder compresses the multivariate time series into a latent representation $z$, which is 
    used for (i) sequence reconstruction by the decoder, (ii) outcome prediction via classification heads, 
    and (iii) representation learning through contrastive projection heads. Losses are combined with 
    task-specific weights, and training includes a warm-up phase to stabilize the latent space.}
    \label{fig:model}
\end{figure}

Finally, predicted logits are converted into \textbf{calibrated probabilities}, using Platt scaling or isotonic regression on the validation set, to produce reliable risk estimates suitable for clinical decision support.
\subsection{Evaluation Modes}

We evaluated the model in two complementary ways:  
(1) directly through the task-specific BCE heads, which serves as our main evaluation and consistently achieved higher accuracy; and  
(2) by applying $k$-nearest neighbors (KNN) classification in the learned contrastive embedding space. While the latter yields lower predictive performance, it provides additional interpretability by allowing a patient’s risk to be assessed in relation to clinically similar neighbors in the embedding space.

\section{Results}

The model was trained on the training set, tuned and calibrated on the validation set, and 
evaluated exclusively on the held-out test set. In this section we report results based on the 
\textbf{BCE head predictions}, which serve as our primary evaluation mode. 
Performance was estimated using bootstrapping on the test cohort, and we report the mean and standard deviation 
of AUROC and AUPR in Fig.~\ref{fig:evaluation}. 

\begin{figure}[h]
    \centering
    {\includegraphics[width=1.02\linewidth]{eval_mortality.png}}\\[0.2cm]
    {\includegraphics[width=1.05\linewidth]{eval_prolonged_stay.png}}\\[0.2cm]
    {\includegraphics[width=1.05\linewidth]{eval_readmission.png}}
    \caption{Evaluation results on the test set for each prediction task. Each panel shows (from left to right) 
    the ROC curve, the precision–recall curve. Mean and standard deviation across 
    bootstrap resamples are reported within each plot.}
    \label{fig:evaluation}
\end{figure}

As expected, the rare outcomes of \textbf{mortality} and especially \textbf{readmission} are the most challenging to predict. Nevertheless, the model maintains good discrimination and calibration, even in these imbalanced settings.  

\subsection{Comparison to baselines}

To understand the contribution of each model component, we compared our full model against 
a plain GRU baseline and ablated versions without reconstruction loss, without supervised contrastive loss, 
and with BCE-only training. 

As shown in the appendix, the model delivers overall consistent improvements over the plain GRU baselines, with the largest gains observed for the rare \textbf{readmission prediction}. 

The results show that each component contributes to final performance: reconstruction improves stability, 
contrastive learning sharpens embeddings especially for rare tasks, and task-specific BCE heads provide 
direct predictive power. The detailed comparison tables (mean AUROC and AUPR across tasks) are reported 
in the Appendix.

\subsection{Feature Importance Evaluation}
\label{sec:featimp}

We assessed feature importance with SHAP (GradientExplainer) applied to our trained model. 
For each task, we wrapped the network to output the task’s calibrated probability and computed SHAP values on 
balanced mini-batches (background from training data, evaluation from test data). We report: 
(i) global importance (mean $|\text{SHAP}|$ across samples and time), 
(ii) temporal heatmaps highlighting when features matter over the first 48\,h, and 
(iii) beeswarm plots aggregating effects over time per feature. 


\section{Discussion}

You should organize your paper into sections and paragraphs to help
readers place a structure on the material and understand its
contributions.

\section{Code Availability}

The implementation of data preprocessing, our model, training and evaluation, is available at our GitHub repository:
bla

\bibliography{reference}
\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\section{Model Details}

\subsection{Architecture}

As explained in section [2.3] The model is a \textbf{multitask sequence autoencoder with supervised contrastive learning}, 
built on a GRU encoder--decoder backbone. The GRU encoder compresses each patient’s 
time series into a latent representation $z$, which is then used for reconstruction of the input sequence (decoder), prediction of the three outcomes via task-specific classification heads, and learning discriminative embeddings via contrastive projection heads.

The total objective is the weighted sum of three complementary losses, described below.

\subsection{Loss Functions}

\paragraph{Reconstruction.}  
To prevent the encoder from collapsing into task-specific shortcuts, we included 
a reconstruction objective that forces $z$ to retain broad information about the 
original time series. We used a masked mean squared error (MSE), which ignores padded values:
\[
    \mathcal{L}_{\text{recon}} = 
    \frac{\sum_{i,t} m_{i,t}\,\|x_{i,t} - \hat{x}_{i,t}\|^2}{\sum_{i,t} m_{i,t}} ,
\]
where $x_{i,t}$ and $\hat{x}_{i,t}$ are the true and reconstructed values for patient $i$ at timestep $t$, 
and $m_{i,t}$ is a binary mask indicating valid timesteps.

\paragraph{Outcome classification.}  
For the three clinical tasks (mortality, prolonged stay, readmission), we used task-specific 
linear heads trained with weighted binary cross-entropy. the BCE loss ${L}_{\text{BCE}}$ is calculated:
\[
    \mathcal - \frac{1}{N}\sum_{i=1}^N \sum_{k=1}^3 w_k
       \Big[ y_{i,k}\log \sigma(\ell_{i,k}) + (1-y_{i,k})\log(1-\sigma(\ell_{i,k})) \Big]
\]
where $\ell_{i,k}$ is the logit for patient $i$ and task $k$, 
$\sigma(\cdot)$ is the sigmoid function, and $w_k = \tfrac{\#\text{neg}_k}{\#\text{pos}_k}$ 
is an inverse class frequency weight to up-weight rare positive cases.  
This prevents the model from being dominated by majority-class negatives.

\paragraph{Supervised Contrastive loss.}  
Each task includes a projection head mapping $z$ into a task-specific embedding space. 
Following (**add paper**) patients with the same label are pulled closer, and those with different labels are pushed apart. We calculate the SupCon loss ${L}_{\text{SupCon}}$:
\[
    \mathcal 
    - \frac{1}{|\mathcal{A}|} \sum_{a \in \mathcal{A}}
    \frac{1}{|\mathcal{P}(a)|} \sum_{p \in \mathcal{P}(a)}
    \log \frac{\exp(z_a \cdot z_p / \tau)}{\sum_{n \in \mathcal{N}(a)} \exp(z_a \cdot z_n / \tau)} ,
\]
where $\mathcal{A}$ are anchors, $\mathcal{P}(a)$ positives for anchor $a$, 
and $\mathcal{N}(a)$ all others. For rare tasks such as mortality and readmission, 
we restricted anchors to positive cases (\textit{positive-only anchors}), ensuring 
that scarce but informative positives shape the embedding space.  
The SupCon terms were also weighted by class frequencies to balance the influence 
of rare versus common outcomes.

\paragraph{Total loss.}  
The three losses were combined into a weighted sum:
\[
    \mathcal{L} = \lambda_{\text{recon}} \mathcal{L}_{\text{recon}} + 
                  \lambda_{\text{BCE}} \mathcal{L}_{\text{BCE}} + 
                  \lambda_{\text{SupCon}} \mathcal{L}_{\text{SupCon}} .
\]
Hyperparameters $\lambda$ control the tradeoff between general representational learning, 
task-specific prediction, and discriminative structure.

\subsection{Training Strategy}

\paragraph{Warm-up.}  
We used a short warm-up phase to stabilize training. Initially, only reconstruction and 
contrastive losses were active 
($\lambda_{\text{recon}}{=}1,\;\lambda_{\text{SupCon}}{=}1,\;\lambda_{\text{BCE}}{=}0$), 
allowing the encoder to build a structured latent space.  
After warm-up, we reduced the reconstruction weight and introduced the BCE loss 
($\lambda_{\text{BCE}}>0$), enabling the model to specialize for prediction.  
This staged approach prevented the classification heads from destabilizing the latent 
space too early.

\paragraph{Pooling.}  
The latent embedding $z$ was built by concatenating:  
(i) the GRU’s final hidden state, which summarizes long-term memory,  
(ii) the mean across time, which captures average trends, and  
(iii) the max across time, which highlights acute peaks.  
This combination allowed $z$ to encode both chronic and acute dynamics.

\paragraph{Batch sampling.}  
We applied a balanced positive-per-task sampler. Each batch explicitly included at least 
six positive cases per outcome, ensuring stable learning signals for rare outcomes. 
The remaining slots were filled with random patients, preserving a realistic distribution 
of the population. This design provided both balance and representativeness.

\paragraph{Calibration.} 
Finally, task logits were post-processed into \textbf{well-calibrated probabilities}, an essential requirement for clinical decision support. 
We applied post-hoc calibration on the validation set using \textit{Platt scaling}, which fits a logistic regression model to map raw logits into calibrated probabilities. 
This substantially improved the alignment between predicted probabilities and observed outcome frequencies. 
As shown in Fig.~\ref{fig:calibration}, calibration reduced deviations from perfect reliability, particularly at intermediate probability ranges, and consistently lowered both expected calibration error (ECE) and Brier scores. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{calibplot_mortality.png}\\[0.2cm]
    \includegraphics[width=\linewidth]{calibplot_prolonged_stay.png}\\[0.2cm]
    \includegraphics[width=\linewidth]{calibplot_readmission.png}
    \caption{Calibration results for the three prediction tasks - mortality, prolonged stay, and readmission (in this order). 
    Each panel compares predictions 
    before (blue) and after (orange) Platt scaling, with reliability curves on the left and 
    probability histograms on the right. Metrics (ECE and Brier score) are reported in each case, confirming improved calibration.}
    \label{fig:calibration}
\end{figure}

\subsection{Hyperparameters}

Model hyperparameters, including the supervised contrastive temperature $\tau$, the loss weights 
($\lambda_{\text{recon}}, \lambda_{\text{BCE}}, \lambda_{\text{SupCon}}$), learning rates, and pooling modes, 
were tuned to balance reconstruction quality, discriminative embedding structure, and predictive performance. 
We explored candidate values both through automated search (Optuna with grid and random sampling strategies) 
and manual refinement. All hyperparameter selection was carried out on the validation set only, 
ensuring that the held-out test set remained untouched for final evaluation.

\subsection{Summary}

In summary, our model combines three synergistic training objectives to shape a joint latent 
representation: reconstruction ensures informativeness, classification provides predictive power, 
and supervised contrastive learning adds discriminative structure. Warm-up training, careful pooling, 
balanced batch sampling, and calibration further improved stability and clinical utility.  
Together, these design choices produced a model that is expressive, robust to imbalance, and capable 
of generating reliable patient-level risk estimates.

\section{Model Evaluation}

\subsection{Ablation Studies and Baseline Comparisons}
\label{app:ablations}

Table~\ref{tab:ablation_bce} reports AUROC and AUPR for predictions from the BCE heads, 
and Table~\ref{tab:ablation_knn} reports results using KNN classification in the 
contrastive embedding space. The results confirm that removing either reconstruction 
or contrastive learning leads to consistent performance drops, especially on the rare readmission task.

\begin{table}[h]
    \centering
    \scriptsize
    \begin{tabular}{lcccccc}
        \toprule
        & \multicolumn{2}{c}{Prolonged stay} & \multicolumn{2}{c}{Mortality} & \multicolumn{2}{c}{Readmission} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
        Model & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR \\
        \midrule
        Baseline GRU & \textbf{0.856} & \textbf{0.431} & \textbf{0.792} & \textbf{0.782} & \textbf{0.691} & \textbf{0.084} \\
        No SupCon    & 0.847 & 0.399 & 0.772 & 0.771 & 0.583 & 0.056 \\
        No Recon     & 0.843 & 0.403 & 0.781 & 0.765 & 0.653 & 0.067 \\
        BCE only     & \textbf{0.857} & 0.409 & 0.779 & 0.770 & 0.583 & 0.064 \\
        SupCon only  & --    & --    & --    & --    & --    & --    \\
        \bottomrule
    \end{tabular}
    \caption{Ablation study: performance of BCE heads on the test set.}
    \label{tab:ablation_bce}
\end{table}

\begin{table}[h]
    \centering
    \scriptsize
    \begin{tabular}{lcccccc}
        \toprule
        & \multicolumn{2}{c}{Prolonged stay} & \multicolumn{2}{c}{Mortality} & \multicolumn{2}{c}{Readmission} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
        Model & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR \\
        \midrule
        Baseline GRU & \textbf{0.797} & \textbf{0.375} & \textbf{0.761} & \textbf{0.737} & \textbf{0.516} & \textbf{0.048} \\
        No SupCon    & 0.758 & 0.299 & 0.750 & 0.722 & 0.500 & 0.045 \\
        No Recon     & 0.766 & 0.326 & 0.749 & 0.717 & 0.501 & 0.043 \\
        BCE only     & 0.792 & 0.320 & 0.739 & 0.715 & 0.521 & 0.051 \\
        SupCon only  & 0.775 & 0.326 & 0.726 & 0.688 & 0.522 & \textbf{0.049} \\
        \bottomrule
    \end{tabular}
    \caption{Ablation study: performance of KNN classification in the contrastive embedding space.}
    \label{tab:ablation_knn}
\end{table}

\subsection{Evaluation Modes}

Our model supports two distinct evaluation strategies:

\paragraph{1. BCE head predictions (primary).}  
The main approach is to use the outputs of the task-specific binary cross-entropy (BCE) heads. 
These are optimized directly for predictive accuracy and probability calibration, and they consistently 
yielded the best performance across AUROC, AUPR, and confusion matrix metrics.  
This is the evaluation mode we report as the primary result in the main paper.  

\paragraph{2. KNN in the embedding space (secondary).}  
In addition, the model produces task-specific embeddings through the supervised contrastive heads. 
By representing each patient in this space, predictions can be made using a $k$-nearest neighbors (KNN) 
classifier: a patient’s outcome is inferred based on the majority label of its closest neighbors.  

While KNN performance in this embedding space is generally lower than the BCE heads (as reported in 
Appendix~\ref{app:ablations}), this method offers \textbf{unique interpretability advantages}. It allows us to 
assess \textit{why} the model considers a patient at risk by explicitly identifying similar past patients 
and their outcomes. This neighborhood-based reasoning can be particularly valuable for exploratory 
analysis and as a complement to calibrated probability estimates.  

\paragraph{Summary.}  
In short, BCE head predictions provide the most reliable quantitative results, while KNN evaluation 
in the embedding space offers a window into the relational structure of the learned representation, 
highlighting clinically meaningful patient similarities.

\subsection{Feature Importance: Methods and Full Results}

\paragraph{Wrapper and model output.}
We adapt the model for SHAP by exposing a task-specific forward pass that returns the probability 
for a single outcome (mortality / prolonged stay / readmission). The wrapper (i) clones the trained model to CPU 
and eval mode, (ii) concatenates the time-step mask as the last input channel, (iii) computes per-sample sequence 
lengths from that mask, and (iv) returns $\sigma(\ell_{k})$ for the chosen task $k$ (probability). 
For stability and determinism during SHAP calls, cuDNN is temporarily disabled around the forward pass.

\paragraph{Background and evaluation sets.}
We form SHAP’s background and evaluation tensors by drawing several balanced batches with our 
\emph{Positive-per-Task} sampler (at least 6 positives per task per batch), using training data for background and test data for evaluation.

\textbf{Evaluation was done using 3 main methods}:

\begin{itemize}
    \item \textbf{Global importance:} Global importance ranks features by \(\text{mean}_{\text{samples},\,t}\,|\text{SHAP}|\).  
    \item \textbf{Temporal patterns (heatmaps):} We compute \(\text{mean}_{\text{samples}}\,|\text{SHAP}|\in\mathbb{R}^{T\times D}\) and display a feature\(\times\)time heatmap for the selected top-$20$ features. We observed the data both in absolute perspective to preserve importance magnitude overall and relative (row-wise 0–1 scaling) to highlight when a feature peaks.  
    \item \textbf{Signed effects (beeswarm):} To summarize signed effects per feature across time, SHAP values are aggregated over the time axis (sum or mean), and a beeswarm plot is drawn on the same top-$20$ features, colored by the mean observed feature value across time.  
\end{itemize}


\subsection{Per-task results and plots}

\paragraph{Notes on interpretability.}
The global bars identify which features matter most overall for each task; the heatmaps indicate \emph{when} 
they matter within the first 48\,h; and the beeswarms reveal the \emph{direction} and spread of effects. 
Together, these complementary views help connect model predictions to clinically meaningful signals.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

